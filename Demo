Perfect üëç ‚Äî since your master sheet already has both startdatetime and enddatetime, we can simplify the script:

Instead of building a ‚Äúwindow‚Äù around a single timestamp, we‚Äôll directly use the exact start and end range from your file.

The script will iterate over each row (id, startdatetime, enddatetime) and collect logs/artifacts in that range.


Here‚Äôs the adapted script (save as activity_reporter.py):

#!/usr/bin/env python3
"""
activity_reporter.py

Reads a master sheet (CSV/XLSX) with columns:
    id,startdatetime,enddatetime
Collects system activity between start and end times and saves a report.

Usage:
    python activity_reporter.py --master master.csv --out ./reports

Requirements:
    pip install pandas openpyxl python-dateutil psutil
"""

import argparse
import os
import sys
import shutil
import subprocess
import traceback
from datetime import datetime
import json
import pandas as pd
from dateutil import parser as dateparser
import psutil
import tempfile
import sqlite3

# -------------------------
# Utilities
# -------------------------
def parse_datetime(s):
    try:
        return dateparser.parse(str(s))
    except Exception:
        return None

def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path

def write_text(path, text, mode="w"):
    with open(path, mode, encoding="utf-8", errors="replace") as f:
        f.write(text)

def run_cmd(cmd, timeout=30):
    try:
        proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        return proc.returncode, proc.stdout, proc.stderr
    except Exception as e:
        return -1, "", str(e)

def format_time(dt):
    return dt.strftime("%Y-%m-%dT%H:%M:%S")

# -------------------------
# Parse master sheet
# -------------------------
def parse_master(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(path)
    else:
        df = pd.read_csv(path)

    if not {"id", "startdatetime", "enddatetime"} <= set(df.columns.str.lower()):
        raise ValueError("Master must have columns: id, startdatetime, enddatetime")

    items = []
    for _, row in df.iterrows():
        ident = str(row["id"])
        start = parse_datetime(row["startdatetime"])
        end = parse_datetime(row["enddatetime"])
        if start and end:
            items.append({"id": ident, "start": start, "end": end})
    return items

# -------------------------
# Collectors (simplified set for demo)
# -------------------------
def collect_system_logs(start, end, out_dir):
    if sys.platform.startswith("linux"):
        cmd = f"journalctl --since='{format_time(start)}' --until='{format_time(end)}' --no-pager"
        rc, out, err = run_cmd(cmd, timeout=60)
        write_text(os.path.join(out_dir, "journalctl.txt"), out + ("\n\n" + err if err else ""))
    elif sys.platform == "win32":
        cmd = f'wevtutil qe System /q:"*[System[TimeCreated[@SystemTime>='{start.isoformat()}Z' and @SystemTime<='{end.isoformat()}Z']]]" /f:text'
        rc, out, err = run_cmd(cmd, timeout=60)
        write_text(os.path.join(out_dir, "windows_events.txt"), out + ("\n\n" + err if err else ""))
    elif sys.platform == "darwin":
        cmd = f"log show --style syslog --start '{format_time(start)}' --end '{format_time(end)}'"
        rc, out, err = run_cmd(cmd, timeout=60)
        write_text(os.path.join(out_dir, "macos_log.txt"), out + ("\n\n" + err if err else ""))

def collect_process_snapshot(out_dir):
    procs = []
    for p in psutil.process_iter(['pid','name','username','create_time','cmdline']):
        try:
            info = p.info
            if info.get("create_time"):
                info["create_time"] = datetime.fromtimestamp(info["create_time"]).isoformat()
            procs.append(info)
        except Exception:
            continue
    write_text(os.path.join(out_dir, "process_snapshot.json"), json.dumps(procs, indent=2))

# -------------------------
# Main flow
# -------------------------
def process_entry(entry, out_base):
    ident, start, end = entry["id"], entry["start"], entry["end"]
    print(f"[+] Processing {ident}: {start} -> {end}")
    folder = ensure_dir(os.path.join(out_base, f"{ident}_{start.strftime('%Y%m%dT%H%M%S')}"))
    meta = {"id": ident, "start": start.isoformat(), "end": end.isoformat()}
    write_text(os.path.join(folder, "meta.json"), json.dumps(meta, indent=2))

    try:
        collect_system_logs(start, end, folder)
        collect_process_snapshot(folder)
    except Exception as e:
        write_text(os.path.join(folder, "error.txt"), traceback.format_exc())

    print(f"[+] Finished {ident}, report at {folder}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--master", required=True, help="Master CSV/XLSX with id,startdatetime,enddatetime")
    parser.add_argument("--out", default="./reports", help="Output folder")
    args = parser.parse_args()

    items = parse_master(args.master)
    out = ensure_dir(args.out)

    for entry in items:
        process_entry(entry, out)

if __name__ == "__main__":
    main()


---

‚úÖ How this version works:

Reads your file with id,startdatetime,enddatetime.

For each row, creates a report folder named after the id + starttime.

Collects:

System logs in that exact time range (journalctl, wevtutil, log show depending on OS).

A current process snapshot (process_snapshot.json).


Saves everything into subfolders under --out.



---

üëâ Question for you:
Do you want me to add back all the deep collectors (browser history, shell history, recent files, DNS cache, etc.) into this version too, but scoped to your start‚Äìend datetimes, or keep it lightweight (logs + processes only)?

